# Awesome Deep Learning Libraries

I have listed some awesome libraries which I found useful for most machine learning practices. These libraries can make 
things easier and boost your productivity.


## General

- [DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples): Source code for many deep learning models by Nvidia


## Computer Vision

- [mmcv](https://github.com/open-mmlab/mmcv): OpenMMLab Computer Vision Foundation
- [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab Image Classification Toolbox and Benchmark
- [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab Detection Toolbox and Benchmark
- [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark
- [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab Semantic Segmentation Toolbox and Benchmark.
- [OpenSelfSup](https://github.com/open-mmlab/OpenSelfSup): Self-Supervised Learning Toolbox and Benchmark


## Natural Language Processing

- [autonlp](https://github.com/huggingface/autonlp): AutoNLP: train state-of-the-art natural language processing models and deploy them in a scalable environment automatically
- [HuggingFaceTransformer](https://github.com/huggingface/transformers): Transformers: State-of-the-art Natural Language Processing for Pytorch, TensorFlow, and JAX.
- [fairseq](https://github.com/pytorch/fairseq): Facebook AI Research Sequence-to-Sequence Toolkit written in Python.


## Data

- [DALI](https://github.com/NVIDIA/DALI): A GPU-accelerated library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications.
- [AugLy](https://github.com/facebookresearch/AugLy): A data augmentations library for audio, image, text, and video.
- [Open3D](https://github.com/intel-isl/Open3D): Open3D: A Modern Library for 3D Data Processing
- [HuggingFaceTokenizer](https://github.com/huggingface/tokenizers): Fast State-of-the-Art Tokenizers optimized for Research and Production
- [HuggingFaceDatasets](https://github.com/huggingface/datasets): The largest hub of ready-to-use NLP datasets for ML models with fast, easy-to-use and efficient data manipulation tools


## Accelerating Training

- [Apex](https://github.com/NVIDIA/apex): A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch
- [ApexDataPrefetcher](https://github.com/NVIDIA/apex/blob/master/examples/imagenet/main_amp.py#L265): prefetch data to hide data I/O cost.
- [Horovod](https://github.com/horovod/horovod): Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.
- [Checkpoint](https://pytorch.org/docs/stable/checkpoint.html): A PyTorch function which implements [activation checkpointing](https://arxiv.org/abs/1604.06174).
- [TorchPipe](https://github.com/kakaobrain/torchgpipe): A GPipe implementation in PyTorch
- [PowerSGD Communication Hook](https://pytorch.org/docs/stable/ddp_comm_hooks.html): PowerSGD (Vogels et al., NeurIPS 2019) is a gradient compression algorithm, which can provide very high compression rates and accelerate bandwidth-bound distributed training. 
- [Accelerate](https://github.com/huggingface/accelerate): A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision
- [lightseq](https://github.com/bytedance/lightseq): LightSeq: A High Performance Library for Sequence Processing and Generation


## Large-Scale Distributed Training

- [Megatron](https://github.com/NVIDIA/Megatron-LM): Ongoing research training transformer language models at scale, including: BERT & GPT-2
- [DeepSpeed](https://github.com/microsoft/deepspeed): DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective.
- [Ray](https://github.com/ray-project/ray): An open source framework that provides a simple, universal API for building distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library.


## Utilities

- [Tensorboard](https://github.com/tensorflow/tensorboard): TensorFlow's Visualization Toolkit
- [KnockKnock](https://github.com/huggingface/knockknock): Get notified when your training ends with only two additional lines of code
- [Neptune](https://github.com/neptune-ai/neptune-client): Lightweight experiment tracking tool for AI/ML individuals and teams. Fits any workflow.
- [netron](https://github.com/lutzroeder/netron): Visualizer for neural network, deep learning, and machine learning models
- [scalene](https://github.com/plasma-umass/scalene): Scalene: a high-performance, high-precision CPU, GPU, and memory profiler for Python


